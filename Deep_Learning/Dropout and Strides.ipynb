{
  "cells": [
    {
      "metadata": {
        "_uuid": "edf754d4b6ca55d07d1e4d3d776dd9154c5944f7"
      },
      "cell_type": "markdown",
      "source": "# Intro\n**This is Lesson 7 in the [Deep Learning](https://www.kaggle.com/learn/deep-learning) course**  \n\nThe models you've built so far have relied on pre-trained models.  But they aren't the ideal solution for many use cases.  In this lesson, you will learn how to build totally new models.\n\n# Lesson\n"
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "_uuid": "5b05b139c5d086b83ab9941f06c46325206a3c58",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\"\"\"\nTransfer Learning only takes you so far. An network trained on millions of photos is not applicable to something like identifying MRIs or picassos.\nHence we need to train our own classifier. Here we will train our dataset on a MNIST dataset, where the columns are index = label, and pixel(No) columns, where No refers to the number\nWe convert the one hot encoded version back into 28 x 28 images\n\nDropout: ignore randomly selected nodes for parts of training, basically preventing overfitting - by preventing more powerful nodes from possibly dominating the overall output\ni.e. each convolution from previous layer must be ignored 50% of time by the following layer. WHEN YOU PREDICT HOWEVER, ALL NODES USED\n\"\"\"",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "'\\n\\n'"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "590f413faa0495cf0fa20517384dd4100d0407bf"
      },
      "cell_type": "markdown",
      "source": "Tra# Sample Code"
    },
    {
      "metadata": {
        "_uuid": "4960cbbc478c5229709f98bddcf468e9b2a0537d",
        "trusted": true
      },
      "cell_type": "code",
      "source": "\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout\n\n\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n#Since the raw here is a csv, and when we call dataPrep we've already read it, we can directly access the \"label\" cikynnb\ndef data_prep(raw):\n    out_y = keras.utils.to_categorical(raw.label, num_classes)\n\n    num_images = raw.shape[0] #Number of rows\n        #Selecting values only, not the label (0-9, which is all in the firstcolumn)\n    x_as_array = raw.values[:,1:] \n    print (x_as_array)\n    print(raw.values[:,:] )\n    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n    #Normalize by 255, which is the map pixel intensities\n    out_x = x_shaped_array / 255\n    return out_x, out_y\n\ntrain_file = \"../input/digit-recognizer/train.csv\"\nraw_data = pd.read_csv(train_file)\n\nx, y = data_prep(raw_data)\n\nmodel = Sequential()\n#Arguments\n#20 = number of convolutions/filters\n#Size of pixels in that convolution filter or kernel_size\n#Flatten is needed to convert into 1D representation\nmodel.add(Conv2D(20, kernel_size=(3, 3),\n                 activation='relu',\n                 input_shape=(img_rows, img_cols, 1)))\nmodel.add(Conv2D(20, kernel_size=(3, 3), activation='relu'))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\nmodel.fit(x, y,\n          batch_size=128,\n          epochs=2,\n          validation_split = 0.2)",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n[[1 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [1 0 0 ... 0 0 0]\n ...\n [7 0 0 ... 0 0 0]\n [6 0 0 ... 0 0 0]\n [9 0 0 ... 0 0 0]]\nTrain on 33600 samples, validate on 8400 samples\nEpoch 1/2\n33600/33600 [==============================] - 35s 1ms/step - loss: 0.2369 - acc: 0.9316 - val_loss: 0.0856 - val_acc: 0.9739\nEpoch 2/2\n33600/33600 [==============================] - 34s 1ms/step - loss: 0.0631 - acc: 0.9809 - val_loss: 0.0575 - val_acc: 0.9813\n",
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "<tensorflow.python.keras.callbacks.History at 0x7fb6fe85db00>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "f73cc312f98d46baaee4add515d4c924a19f0860"
      },
      "cell_type": "markdown",
      "source": "# Your Turn\nYou are ready to **[build your own model](https://www.kaggle.com/kernels/fork/574269)**.\n\n---\n**[Deep Learning Course Home Page](https://www.kaggle.com/learn/deep-learning)**\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}